{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2eb6c555",
   "metadata": {},
   "source": [
    "# Text analysis tasks with ChatGPT API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ba4bb0",
   "metadata": {},
   "source": [
    "## Text analysis tasks:\n",
    "\n",
    "- Text summarization\n",
    "- Extraction of topics, named entities, etc.\n",
    "- Sentiment analysis\n",
    "- Translation to other languages\n",
    "- Rephrasing to correct or address a need"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c94dae",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b0d42b",
   "metadata": {},
   "source": [
    "### *Imports and declarations*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a56732c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import wikipedia\n",
    "import tiktoken\n",
    "from langchain import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "llm_model = OpenAI(temperature=0.0)\n",
    "\n",
    "tokenizer = tiktoken.encoding_for_model(llm_model.model_name)\n",
    "\n",
    "# Cost of executing ChatGPT calls is accumulated in 'total_cost'\n",
    "# Summary is printed at the end of this notebook\n",
    "total_cost = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e3a24e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468e067c",
   "metadata": {},
   "source": [
    "### Summarize Wikipedia article on GPT-3 \n",
    "\n",
    "Python Wikipedia library documentation: https://wikipedia.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e42acf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(text, length, llm=llm_model, print_full_prompt=False):\n",
    "    # text and length must be valid strings, length should be a string representation of an integer\n",
    "    global total_cost\n",
    "    \n",
    "    summarization_template_string = \"\"\"\n",
    "    Summarize the text delimited by tripple backticks in {length} words.\\\n",
    "    text: ```{text}```\n",
    "    \"\"\"\n",
    "    summarization_prompt_template = PromptTemplate(\n",
    "        input_variables=[\"text\", \"length\"],\n",
    "        template=summarization_template_string\n",
    "    )\n",
    "    \n",
    "    model_input = summarization_prompt_template.format(text=text, length=length)\n",
    "\n",
    "    if print_full_prompt:\n",
    "        print(f\"Full prompt:\\n{model_input}\\n\")\n",
    "    \n",
    "    with get_openai_callback() as cb:\n",
    "        response = llm(model_input)\n",
    "        \n",
    "    total_cost += cb.total_cost\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9e8a45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wikipedia page on GPT-3: https://en.wikipedia.org/wiki/GPT-3\n",
    "\n",
    "wikipedia.set_lang(\"en\")\n",
    "gpt3_article = wikipedia.page(\"GPT-3\", auto_suggest=False).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6e80fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generative Pre-trained Transformer 3 (GPT-3) is a large language model released by OpenAI in 2020. Like its predecessor GPT-2, it is a decoder-only transformer model of deep neural network, which uses attention in place of previous recurrence- and convolution-based architectures. Attention mechanisms allow the model to selectively focus on segments of input text it predicts to be the most relevant. It uses a 2048-tokens-long context and then-unprecedented size of 175 billion parameters, requirin\n"
     ]
    }
   ],
   "source": [
    "print(gpt3_article[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48d333c",
   "metadata": {},
   "source": [
    "Check the article lenght in tokens to assure it fits into LLM's input limitation (together with prompt template text), which is 4096 tokens for GPT-3.5-Turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2418b8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3684"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.encode(gpt3_article))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eae64ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full prompt:\n",
      "\n",
      "    Summarize the text delimited by tripple backticks in 200 words.    text: ```Generative Pre-trained Transformer 3 (GPT-3) is a large language model released by OpenAI in 2020. Like its predecessor GPT-2, it is a decoder-only transformer model of deep neural network, which uses attention in place of previous recurrence- and convolution-based architectures. Attention mechanisms allow the model to selectively focus on segments of input text it predicts to be the most relevant. It uses a 2048-tokens-long context and then-unprecedented size of 175 billion parameters, requiring 800GB to store. The model demonstrated strong zero-shot and few-shot learning on many tasks.Microsoft announced on September 22, 2020, that it had licensed \"exclusive\" use of GPT-3; others can still use the public API to receive output, but only Microsoft has access to GPT-3's underlying model.\n",
      "\n",
      "\n",
      "== Background ==\n",
      "According to The Economist, improved algorithms, powerful computers, and an increase in digitized data have fueled a revolution in machine learning, with new techniques in the 2010s resulting in \"rapid improvements in tasks\" including manipulating language. Software models are trained to learn by using thousands or millions of examples in a \"structure ... loosely based on the neural architecture of the brain\". One architecture used in natural language processing (NLP) is a neural network based on a deep learning model that was first introduced in 2017—the transformer architecture. There are a number of NLP systems capable of processing, mining, organizing, connecting and contrasting textual input, as well as correctly answering questions.On June 11, 2018, OpenAI researchers and engineers posted their original paper introducing the first generative pre-trained transformer (GPT)—a type of generative large language model that is pre-trained with an enormous and diverse corpus of text via datasets, followed by discriminative fine-tuning to focus on a specific task. GPT models are transformer-based deep learning neural network architectures. Up to that point, the best-performing neural NLP models commonly employed supervised learning from large amounts of manually-labeled data, which made it prohibitively expensive and time-consuming to train extremely large language models.\n",
      "That first GPT model is known as \"GPT-1,\" and it was then followed by \"GPT-2\" in February 2019. GPT-2 was created as a direct scale-up of GPT-1, with both its parameter count and dataset size increased by a factor of 10. It had 1.5 billion parameters, and was trained on a dataset of 8 million web pages.In February 2020, Microsoft introduced its Turing Natural Language Generation (T-NLG), which was claimed to be the \"largest language model ever published at 17 billion parameters.\" It performed better than any other language model at a variety of tasks which included summarizing texts and answering questions.\n",
      "\n",
      "\n",
      "== Training and capabilities ==\n",
      "\n",
      "On May 28, 2020, an arXiv preprint by a group of 31 engineers and researchers at OpenAI described the achievement and development of GPT-3, a third-generation \"state-of-the-art language model\". The team increased the capacity of GPT-3 by over two orders of magnitude from that of its predecessor, GPT-2, making GPT-3 the largest non-sparse language model to date.: 14  Because GPT-3 is structurally similar to its predecessors, its greater accuracy is attributed to its increased capacity and greater number of parameters. GPT-3's capacity is ten times larger than that of Microsoft's Turing NLG, the next largest NLP model known at the time.Lambdalabs estimated a hypothetical cost of around $4.6 million US dollars and 355 years to train GPT-3 on a single GPU in 2020, with lower actual training time by using more GPUs in parallel.\n",
      "Sixty percent of the weighted pre-training dataset for GPT-3 comes from a filtered version of Common Crawl consisting of 410 billion byte-pair-encoded tokens.: 9  Other sources are 19 billion tokens from WebText2 representing 22% of the weighted total, 12 billion tokens from Books1 representing 8%, 55 billion tokens from Books2 representing 8%, and 3 billion tokens from Wikipedia representing 3%.: 9  GPT-3 was trained on hundreds of billions of words and is also capable of coding in CSS, JSX, and Python, among others.\n",
      "Since GPT-3's training data was all-encompassing, it does not require further training for distinct language tasks. The training data contains occasional toxic language and GPT-3 occasionally generates toxic language as a result of mimicking its training data. A study from the University of Washington found that GPT-3 produced toxic language at a toxicity level comparable to the similar natural language processing models of GPT-2 and CTRL. OpenAI has implemented several strategies to limit the amount of toxic language generated by GPT-3. As a result, GPT-3 produced less toxic language compared to its predecessor model, GPT-1, although it produced both more generations and a higher toxicity of toxic language compared to CTRL Wiki, a language model trained entirely on Wikipedia data.On June 11, 2020, OpenAI announced that users could request access to its user-friendly GPT-3 API—a \"machine learning toolset\"—to help OpenAI \"explore the strengths and limits\" of this new technology. The invitation described how this API had a general-purpose \"text in, text out\" interface that can complete almost \"any English language task\", instead of the usual single use-case. According to one user, who had access to a private early release of the OpenAI GPT-3 API, GPT-3 was \"eerily good\" at writing \"amazingly coherent text\" with only a few simple prompts. In an initial experiment 80 US subjects were asked to judge if short ~200 word articles were written by humans or GPT-3. The participants judged correctly 52% of the time, doing only slightly better than random guessing.On November 18, 2021, OpenAI announced that enough safeguards had been implemented that access to its API would be unrestricted. OpenAI provided developers with a content moderation tool that helps them abide by OpenAI's content policy. On January 27, 2022, OpenAI announced that its newest GPT-3 language models (collectively referred to as InstructGPT) were now the default language model used on their API. According to OpenAI, InstructGPT produced content that was better aligned to user intentions by following instructions better, generating fewer made-up facts, and producing somewhat less toxic content.Because GPT-3 can \"generate news articles which human evaluators have difficulty distinguishing from articles written by humans,\" GPT-3 has the \"potential to advance both the beneficial and harmful applications of language models.\": 34  In their May 28, 2020 paper, the researchers described in detail the potential \"harmful effects of GPT-3\" which include \"misinformation, spam, phishing, abuse of legal and governmental processes, fraudulent academic essay writing and social engineering pretexting\". The authors draw attention to these dangers to call for research on risk mitigation.: 34 GPT-3 is capable of performing zero-shot and few-shot learning (including one-shot).In June 2022, Almira Osmanovic Thunström wrote that GPT-3 was the primary author on an article on itself, that they had submitted it for publication, and that it had been pre-published while waiting for completion of its review.\n",
      "\n",
      "\n",
      "== InstructGPT ==\n",
      "InstructGPT is a finetuned version of GPT-3. It has been trained on a dataset of human-written instructions. This training allows InstructGPT to better understand what is being asked of it, and to generate more accurate and relevant outputs. \n",
      "\n",
      "InstructGPT can be used to follow instructions that are given in natural language.\n",
      "InstructGPT can be used to answer questions that are asked in natural language.\n",
      "InstructGPT is more accurate and relevant than GPT-3 when following instructions and answering questions.\n",
      "InstructGPT can be used in a variety of applications, such as customer service, education, and automation.\n",
      "\n",
      "\n",
      "== GPT-3 models ==\n",
      "There are many models in the GPT-3 family, some serving different purposes than others. In the initial research paper published by OpenAI, they mentioned 8 different sizes of the main GPT-3 model:\n",
      "\n",
      "Half of the models are accessible through the API, namely GPT-3-medium, GPT-3-xl, GPT-3-6.7B and GPT-3-175b, which are referred to as ada, babbage, curie and davinci respectively.\n",
      "\n",
      "\n",
      "== GPT-3.5 ==\n",
      "Generative Pre-trained Transformer 3.5 (GPT-3.5) is a sub class of GPT-3 Models created by OpenAI in 2022.\n",
      "On March 15, 2022, OpenAI made available new versions of GPT-3 and Codex in its API with edit and insert capabilities under the names \"text-davinci-002\" and \"code-davinci-002\". These models were described as more capable than previous versions and were trained on data up to June 2021. On November 28, 2022, OpenAI introduced text-davinci-003. On November 30, 2022, OpenAI began referring to these models as belonging to the \"GPT-3.5\" series, and released ChatGPT, which was fine-tuned from a model in the GPT-3.5 series. OpenAI does not include GPT-3.5 in GPT-3.\n",
      "\n",
      "\n",
      "=== Models ===\n",
      "There are four models:\n",
      "Chat\n",
      "gpt-3.5-turbo\n",
      "Text completion\n",
      "text-davinci-003\n",
      "text-davinci-002\n",
      "\n",
      "\n",
      "=== GPT-3.5 with browsing ===\n",
      "On April 10, 2023, OpenAI introduced a new variant of its GPT-3.5 series model, known as GPT-3.5 with Browsing (ALPHA). This updated model was described to build upon the capabilities of its predecessors \"text-davinci-002\" and \"code-davinci-002\". The GPT-3.5 with Browsing (ALPHA) model incorporated the ability to access and browse online information. This has led to more accurate and up-to-date responses to user queries.The GPT-3.5 with Browsing (ALPHA) model has been trained on data up to September 2021, giving it more information compared to previous GPT-3.5 models, which were trained on data up until June 2021. The model attempted to provide developers and users with an advanced natural language processing tool that can effectively retrieve and synthesize online information.To enable browsing capabilities, OpenAI implemented a new API that allows the GPT-3.5 with Browsing (ALPHA) model to access selected online resources during operation. This feature allows users to ask questions or request information with the expectation that the model will deliver updated, accurate, and relevant answers based on the latest online sources available to it.\n",
      "On April 27, 2023, OpenAI made the GPT-3.5 with Browsing (ALPHA) model publicly available to GPT Plus users. This allowed more people to access to its new features.\n",
      "\n",
      "\n",
      "== Reception ==\n",
      "\n",
      "\n",
      "=== Applications ===\n",
      "GPT-3, specifically the Codex model, is the basis for GitHub Copilot, a code completion and generation software that can be used in various code editors and IDEs.\n",
      "GPT-3 is used in certain Microsoft products to translate conventional language into formal computer code.\n",
      "GPT-3 has been used in CodexDB to generate query-specific code for SQL processing.\n",
      "GPT-3 has been used by Jason Rohrer in a retro-themed chatbot project named \"Project December\", which is accessible online and allows users to converse with several AIs using GPT-3 technology.\n",
      "GPT-3 was used by The Guardian to write an article about AI being harmless to human beings. It was fed some ideas and produced eight different essays, which were ultimately merged into one article.\n",
      "GPT-3 was used in AI Dungeon, which generates text-based adventure games. Later it was replaced by a competing model after OpenAI changed their policy regarding generated content.\n",
      "GPT-3 is used to aid in writing copy and other marketing materials.\n",
      "A 2022 study from Drexel University suggested that GPT-3-based systems could be used to screen for early signs of Alzheimer's disease.\n",
      "\n",
      "\n",
      "=== Reviews ===\n",
      "In a July 2020 review in The New York Times, Farhad Manjoo said that GPT-3's ability to generate computer code, poetry, and prose is not just \"amazing\", \"spooky\", and \"humbling\", but also \"more than a little terrifying\".\n",
      "Daily Nous presented a series of articles by nine philosophers on GPT-3. Australian philosopher David Chalmers described GPT-3 as \"one of the most interesting and important AI systems ever produced\".\n",
      "A review in Wired said that GPT-3 was \"provoking chills across Silicon Valley\".\n",
      "The National Law Review said that GPT-3 is an \"impressive step in the larger process\", with OpenAI and others finding \"useful applications for all of this power\" while continuing to \"work toward a more general intelligence\".\n",
      "An article in the MIT Technology Review, co-written by Deep Learning critic Gary Marcus, stated that GPT-3's \"comprehension of the world is often seriously off, which means you can never really trust what it says.\" According to the authors, GPT-3 models relationships between words without having an understanding of the meaning behind each word.\n",
      "Jerome Pesenti, head of the Facebook AI lab, said GPT-3 is \"unsafe,\" pointing to the sexist, racist and other biased and negative language generated by the system when it was asked to discuss Jews, women, black people, and the Holocaust.\n",
      "Nabla, a French start-up specializing in healthcare technology, tested GPT-3 as a medical chatbot, though OpenAI itself warned against such use. As expected, GPT-3 showed several limitations. For example, while testing GPT-3 responses about mental health issues, the AI advised a simulated patient to commit suicide.\n",
      "Noam Chomsky expressed his skepticism about GPT-3's scientific value: \"It's not a language model. It works just as well for impossible languages as for actual languages. It is therefore refuted, if intended as a language model, by normal scientific criteria. [...] Perhaps it's useful for some purpose, but it seems to tell us nothing about language or cognition generally.\"\n",
      "Luciano Floridi and Massimo Chiriatti highlighted the risk of \"cheap production of good, semantic artefacts\".\n",
      "OpenAI's Sam Altman himself criticized what he called \"GPT-3 hype\", acknowledging GPT-3 \"has serious weakness and sometimes makes very silly mistakes... AI is going to change the world, but GPT-3 is just a very early glimpse.\"\n",
      "\n",
      "\n",
      "=== Criticism ===\n",
      "GPT-3's builder, OpenAI, was initially founded as a non-profit in 2015. In 2019, OpenAI broke from its usual open-source standards by not publicly releasing GPT-3's predecessor model, citing concerns that the model could facilitate the propagation of fake news. OpenAI eventually released a version of GPT-2 that was 8% of the original model's size. In the same year, OpenAI restructured to be a for-profit company. In 2020, Microsoft announced the company had exclusive licensing of GPT-3 for Microsoft's products and services following a multi-billion dollar investment in OpenAI. The agreement permits OpenAI to offer a public-facing API such that users can send text to GPT-3 to receive the model's output, but only Microsoft will have access to GPT-3's source code.Large language models, such as GPT-3, have come under criticism from a few of Google's AI ethics researchers for the environmental impact of training and storing the models, detailed in a paper co-authored by Timnit Gebru and Emily M. Bender in 2021.The growing use of automated writing technologies based on GPT-3 and other language generators, has raised concerns regarding academic integrity and raised the stakes of how universities and schools will gauge what constitutes academic misconduct such as plagiarism.OpenAI's GPT series was built with data from the Common Crawl dataset, a conglomerate of copyrighted articles, internet posts, web pages, and books scraped from 60 million domains over a period of 12 years. TechCrunch reports this training data includes copyrighted material from the BBC, The New York Times, Reddit, the full text of online books, and more. In its response to a 2019 Request for Comments on Intellectual Property Protection for Artificial Intelligence Innovation from the United States Patent and Trademark Office (USPTO), OpenAI argued that \"Under current law, training AI systems [such as its GPT models] constitutes fair use,\" but that \"given the lack of case law on point, OpenAI and other AI developers like us face substantial legal uncertainty and compliance costs.\"\n",
      "\n",
      "\n",
      "== See also ==\n",
      "BERT (language model)\n",
      "Hallucination (artificial intelligence)\n",
      "LaMDA\n",
      "Wu Dao\n",
      "GPT-4\n",
      "\n",
      "\n",
      "== References ==```\n",
      "    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "gpt3_summary = summarize(gpt3_article, length=\"200\", print_full_prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6186d29b-1655-4f4f-9878-eae6fe4261a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      "\n",
      "Generative Pre-trained Transformer 3 (GPT-3) is a large language model released by OpenAI in 2020. It is a decoder-only transformer model of deep neural network, which uses attention in place of previous recurrence- and convolution-based architectures. GPT-3 has an unprecedented size of 175 billion parameters, requiring 800GB to store. It demonstrated strong zero-shot and few-shot learning on many tasks. Microsoft licensed exclusive use of GPT-3, while others can still use the public API to receive output.\n",
      "\n",
      "GPT-3 is trained on hundreds of billions of words and is capable of coding in CSS, JSX, and Python, among others. It does not require further training for distinct language tasks, but it occasionally generates toxic language as a result of mimicking its training data. OpenAI has implemented strategies to limit the amount of toxic language generated by GPT-3.\n",
      "\n",
      "InstructGPT is a finetuned version of GPT-3. It has been trained on a dataset of human-written instructions, allowing it to better understand what is being asked of it and to generate more accurate and relevant outputs.\n",
      "\n",
      "GPT-3.5 is a sub class of GPT\n"
     ]
    }
   ],
   "source": [
    "print(f\"Summary:\\n{gpt3_summary}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b71af9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count words in the summary\n",
    "import re\n",
    "\n",
    "len(re.findall(r'\\w+', gpt3_summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56d9dbbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count tokens in the summary\n",
    "\n",
    "len(tokenizer.encode(gpt3_summary))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bab62e8",
   "metadata": {},
   "source": [
    "### Summarize Wikipedia article on GPT-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "32d6b3f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3342"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Wikipedia page on GPT-4: https://en.wikipedia.org/wiki/GPT-4\n",
    "\n",
    "gpt4_article = wikipedia.page(\"GPT-4\", auto_suggest=False).content\n",
    "len(tokenizer.encode(gpt4_article))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b8b2ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      "\n",
      "Generative Pre-trained Transformer 4 (GPT-4) is a large language model created by OpenAI, and the fourth in its series of GPT foundation models. It was initially released on March 14, 2023, and has been made publicly available via the paid chatbot product ChatGPT Plus, and via OpenAI's API. GPT-4 is a transformer-based model, which uses pre-training on public data and \"data licensed from third-party providers\" to predict the next token. After this step, the model was then fine-tuned with reinforcement learning feedback from humans and AI for human alignment and policy compliance.\n",
      "\n",
      "Observers reported that the iteration of ChatGPT using GPT-4 was an improvement on the previous iteration based on GPT-3.5, with the caveat that GPT-4 retains some of the problems with earlier revisions. GPT-4 is also capable of taking images as input, though this feature has not been made available since launch. OpenAI has declined to reveal various technical details and statistics about GPT-4, such as the precise size of the model.\n",
      "\n",
      "GPT-4 is capable of performing various tasks with few examples, such as using APIs,\n"
     ]
    }
   ],
   "source": [
    "gpt4_summary = summarize(gpt4_article, length=\"200\")\n",
    "    \n",
    "print(f\"Summary:\\n{gpt4_summary}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "adc23d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      "\n",
      "OpenAI's Generative Pre-trained Transformer 4 (GPT-4) is a large language model released in March 2023. It is a transformer-based model that uses pre-training and reinforcement learning to predict the next token. GPT-4 is a multimodal model, capable of taking images as input, and is used in products such as ChatGPT Plus and Microsoft Bing. It has been tested on standardized tests and medical problems, and has been found to be useful for coding tasks. It has been criticized for its lack of transparency and potential biases, and safety concerns have been raised due to its ability to hallucinate and respond to harmful prompts. GPT-4 is used in products such as ChatGPT Plus, Microsoft Bing, Copilot, Duolingo, Khan Academy, Be My Eyes, and Stripe.\n"
     ]
    }
   ],
   "source": [
    "short_gpt4_summary = summarize(gpt4_article, length=\"100\")\n",
    "    \n",
    "print(f\"Summary:\\n{short_gpt4_summary}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f65fef9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count words in the short summary\n",
    "\n",
    "len(re.findall(r'\\w+', short_gpt4_summary))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05162eb7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b69b334",
   "metadata": {},
   "source": [
    "## Extract topics, named entities, etc. from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0981856b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(text, topic, llm=llm_model):\n",
    "    # text and topics must be valid strings\n",
    "    global total_cost\n",
    "    \n",
    "    extraction_template_string = \"\"\"\n",
    "    Extract {topic} from the text delimited by tripple backticks.\\\n",
    "    text: ```{text}```\n",
    "    \"\"\"\n",
    "    extraction_prompt_template = PromptTemplate.from_template(extraction_template_string)\n",
    "    \n",
    "    model_input = extraction_prompt_template.format(text=text, topic=topic)\n",
    "\n",
    "    with get_openai_callback() as cb:\n",
    "        response = llm(model_input)\n",
    "        \n",
    "    total_cost += cb.total_cost\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d6e87aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Main Topic: GPT-3 and its variants\n"
     ]
    }
   ],
   "source": [
    "print(extract(gpt3_summary, \"main topic\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bdb7b94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-v8G8kOQ3lnYLEW1SXIh6q9ej on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-v8G8kOQ3lnYLEW1SXIh6q9ej on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Main topic: GPT-4\n"
     ]
    }
   ],
   "source": [
    "print(extract(gpt4_summary, \"main topic\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ee449dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-v8G8kOQ3lnYLEW1SXIh6q9ej on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-v8G8kOQ3lnYLEW1SXIh6q9ej on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-v8G8kOQ3lnYLEW1SXIh6q9ej on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-v8G8kOQ3lnYLEW1SXIh6q9ej on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "GPT-3, InstructGPT, GPT-3.5\n"
     ]
    }
   ],
   "source": [
    "print(extract(gpt3_summary, \"list of model names\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f4a5c032",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-v8G8kOQ3lnYLEW1SXIh6q9ej on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-v8G8kOQ3lnYLEW1SXIh6q9ej on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-v8G8kOQ3lnYLEW1SXIh6q9ej on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mextract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgpt4_summary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlist of applications\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[1;32mIn[21], line 14\u001b[0m, in \u001b[0;36mextract\u001b[1;34m(text, topic, llm)\u001b[0m\n\u001b[0;32m     11\u001b[0m model_input \u001b[38;5;241m=\u001b[39m extraction_prompt_template\u001b[38;5;241m.\u001b[39mformat(text\u001b[38;5;241m=\u001b[39mtext, topic\u001b[38;5;241m=\u001b[39mtopic)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_openai_callback() \u001b[38;5;28;01mas\u001b[39;00m cb:\n\u001b[1;32m---> 14\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m total_cost \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m cb\u001b[38;5;241m.\u001b[39mtotal_cost\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\llms\\base.py:831\u001b[0m, in \u001b[0;36mBaseLLM.__call__\u001b[1;34m(self, prompt, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(prompt, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    825\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    826\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument `prompt` is expected to be a string. Instead found \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    827\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(prompt)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. If you want to run the LLM on multiple prompts, use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    828\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`generate` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    829\u001b[0m     )\n\u001b[0;32m    830\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 831\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    832\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    833\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    834\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    835\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    836\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    837\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    838\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    839\u001b[0m     \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    840\u001b[0m     \u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m    841\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\llms\\base.py:627\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[0;32m    618\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    619\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    620\u001b[0m         )\n\u001b[0;32m    621\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    622\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[0;32m    623\u001b[0m             dumpd(\u001b[38;5;28mself\u001b[39m), [prompt], invocation_params\u001b[38;5;241m=\u001b[39mparams, options\u001b[38;5;241m=\u001b[39moptions\n\u001b[0;32m    624\u001b[0m         )[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    625\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m callback_manager, prompt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(callback_managers, prompts)\n\u001b[0;32m    626\u001b[0m     ]\n\u001b[1;32m--> 627\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    629\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\llms\\base.py:529\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    527\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[0;32m    528\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e)\n\u001b[1;32m--> 529\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    530\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\llms\\base.py:516\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    506\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[0;32m    507\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    508\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    513\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    514\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    515\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 516\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    517\u001b[0m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    519\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[0;32m    520\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    523\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    524\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[0;32m    525\u001b[0m         )\n\u001b[0;32m    526\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    527\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\llms\\openai.py:387\u001b[0m, in \u001b[0;36mBaseOpenAI._generate\u001b[1;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    375\u001b[0m     choices\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m    376\u001b[0m         {\n\u001b[0;32m    377\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: generation\u001b[38;5;241m.\u001b[39mtext,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    384\u001b[0m         }\n\u001b[0;32m    385\u001b[0m     )\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 387\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mcompletion_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    388\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_prompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\n\u001b[0;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    390\u001b[0m     choices\u001b[38;5;241m.\u001b[39mextend(response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    391\u001b[0m     update_token_usage(_keys, response, token_usage)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\llms\\openai.py:115\u001b[0m, in \u001b[0;36mcompletion_with_retry\u001b[1;34m(llm, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_completion_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m llm\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 115\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_completion_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tenacity\\__init__.py:289\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_f\u001b[39m(\u001b[38;5;241m*\u001b[39margs: t\u001b[38;5;241m.\u001b[39mAny, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: t\u001b[38;5;241m.\u001b[39mAny) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m t\u001b[38;5;241m.\u001b[39mAny:\n\u001b[1;32m--> 289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tenacity\\__init__.py:389\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoSleep):\n\u001b[0;32m    388\u001b[0m     retry_state\u001b[38;5;241m.\u001b[39mprepare_for_next_attempt()\n\u001b[1;32m--> 389\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    390\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    391\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m do\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tenacity\\nap.py:31\u001b[0m, in \u001b[0;36msleep\u001b[1;34m(seconds)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msleep\u001b[39m(seconds: \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     26\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;124;03m    Sleep strategy that delays execution for a given number of seconds.\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \n\u001b[0;32m     29\u001b[0m \u001b[38;5;124;03m    This is the default strategy, and may be mocked out for unit testing.\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(seconds)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(extract(gpt4_summary, \"list of applications\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a24598",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0648b20",
   "metadata": {},
   "source": [
    "## Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6288708",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis(text, llm=llm_model):\n",
    "    global total_cost\n",
    "    \n",
    "    sentiment_template_string = \"\"\"\n",
    "    Classify the sentiment expressed in the review delimited by tripple backticks.\\\n",
    "    review: ```{text}```\n",
    "    \"\"\"\n",
    "    sentiment_prompt_template = PromptTemplate.from_template(sentiment_template_string)\n",
    "\n",
    "    model_input = sentiment_prompt_template.format(text=text)\n",
    "\n",
    "    with get_openai_callback() as cb:\n",
    "        response = llm(model_input)\n",
    "        \n",
    "    total_cost += cb.total_cost\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ea2206",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_1 = \"\"\"\n",
    "I purchased the PixelPioneer Quantum 60\" and it's a game-changer.\n",
    "The 4K resolution is stunning and the smart features are easy to use.\n",
    "Worth every penny! - George, Liverpool\"\"\"\n",
    "\n",
    "print(sentiment_analysis(review_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42abc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_2 = \"\"\"\n",
    "I'm not happy with the VisionCast UltraView 43\".\n",
    "The picture quality is subpar and the TV arrived with a scratch on the screen.\n",
    "I expected better quality control. - Sarah, Los Angeles\"\"\"\n",
    "\n",
    "print(sentiment_analysis(review_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f721a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_3 = \"\"\"\n",
    "I bought the PixelPioneer Quantum 70\" and it's simply fantastic.\n",
    "The voice control remote is a game-changer.\n",
    "However, the delivery was delayed by a week which was quite frustrating. - Emma, London\"\"\"\n",
    "\n",
    "print(sentiment_analysis(review_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4702ae9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c84477d",
   "metadata": {},
   "source": [
    "## Translation to other languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f186a4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(text, target_language, llm=llm_model):\n",
    "    global total_cost\n",
    "    \n",
    "    translation_template_string = \"\"\"\n",
    "    Translate the text delimited by tripple backticks into {language}.\\\n",
    "    text: ```{text}```\n",
    "    \"\"\"\n",
    "    translation_prompt_template = PromptTemplate.from_template(translation_template_string)\n",
    "    \n",
    "    model_input = translation_prompt_template.format(text=text, language=target_language)\n",
    "\n",
    "    with get_openai_callback() as cb:\n",
    "        response = llm(model_input)\n",
    "        \n",
    "    total_cost += cb.total_cost\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6a4451",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_text = \"Some of the capabilities of GPT-4 include describing humor in images, \\\n",
    "summarizing text from screenshots, and answering exam questions with diagrams.\"\n",
    "\n",
    "spanish_translation = translate(english_text, \"Spanish\")\n",
    "\n",
    "print(spanish_translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca8c88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "italian_translation = translate(english_text, \"Italian\")\n",
    "\n",
    "print(italian_translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef104f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(translate(spanish_translation, \"Italian\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b487d840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quote from Wikipedia: https://el.wikipedia.org/wiki/GPT-4\n",
    "\n",
    "greek_text = \"Ως μετασχηματιστής, το GPT-4 ήταν προεκπαιδευμένο για την πρόβλεψη του επόμενου διακριτικού \\\n",
    "(χρησιμοποιώντας δημόσια δεδομένα και «δεδομένα με άδεια από τρίτους παρόχους») και στη συνέχεια βελτιστοποιήθηκε \\\n",
    "με ενισχυτική μάθηση από την ανάδραση ανθρώπου και τεχνητής νοημοσύνης για ανθρώπινη ευθυγράμμιση και πολιτική συμμόρφωση.\"\n",
    "\n",
    "print(translate(greek_text, \"English\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325f2fb5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a1f9f8",
   "metadata": {},
   "source": [
    "## Rephrasing to correct or address a need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb85dd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_text(text, llm=llm_model):\n",
    "    global total_cost\n",
    "    \n",
    "    correct_grammar_template_string = \"\"\"\n",
    "    Correct grammar, punctuation and spelling in the text delimited by tripple backticks.\\\n",
    "    text: ```{text}```\n",
    "    \"\"\"\n",
    "    correct_grammar_prompt_template = PromptTemplate.from_template(correct_grammar_template_string)\n",
    "    \n",
    "    model_input = correct_grammar_prompt_template.format(text=text)\n",
    "\n",
    "    with get_openai_callback() as cb:\n",
    "        response = llm(model_input)\n",
    "        \n",
    "    total_cost += cb.total_cost\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fc411e",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_text = \"\"\"\n",
    "The model has limitations, including the tendency to hallucinate and lack transparency\n",
    "in its decision-making processes. It has also been found to have cognitive biases.\"\"\"\n",
    "\n",
    "altered_text = \"\"\"\n",
    "The mdel has limmitaions including, the tendency to halucinate and lsck trespacy\n",
    "in its decision making processes. It has also been fond to hav cognitive biasses.\"\"\"\n",
    "\n",
    "print(correct_text(altered_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47529fd5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dad0fda",
   "metadata": {},
   "source": [
    "## Examples of other ChatGPT based applications:\n",
    "\n",
    "- Chatbots\n",
    "- Question answering over documents\n",
    "- Customer support agents\n",
    "- Querying and analyzis of structured data\n",
    "- Personal assistants, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86a0fb4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aee4680",
   "metadata": {},
   "source": [
    "# Get the total cost of running ChatGPT API calls in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3bdd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total cost: ${total_cost:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3914c79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
